{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPntO3o4KgJ9"
      },
      "source": [
        "## SECOM Data Set Information\n",
        "\n",
        "A complex modern semi-conductor manufacturing process is normally under consistent surveillance via the monitoring of signals/variables collected from sensors and or process measurement points. However, not all of these signals are equally valuable in a specific monitoring system. The measured signals contain a combination of useful information, irrelevant information as well as noise. It is often the case  that useful information is buried in the latter two. Engineers typically have a much larger number of signals than are actually required. If we consider each type  of signal as a feature, then feature selection may be applied to identify the most relevant signals. The Process Engineers may then use these signals to determine key factors contributing to yield excursions downstream in the process. This will enable an increase in process throughput, decreased time to learning and reduce the per unit production costs.\n",
        "\n",
        "Numerical data are recorded values from a series of sensors in the production machines that are placed in specified locations to help identify the part of the production process which contributes to the faults.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeDNsh2nKgKA"
      },
      "source": [
        "# Objective\n",
        "To minimize the rate at which faulty products leave the factory, the numerical data starts to make sense.\n",
        "\n",
        "*   To enhance current business improvement techniques, we use feature selection techniques to rank features according to their impact on the overall yield for the product.\n",
        "\n",
        "    *   Causal relationships may also be considered with a view to identifying the key features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_GFhyT1KgKA"
      },
      "source": [
        "Dimensionality reduction techniques:\n",
        "\n",
        "- Percent Missing Values\n",
        "- Ammount of Variation\n",
        "- Pairwise Correlation\n",
        "- Correlation with Target\n",
        "- Recursive feature elimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9qcftnLKgKE"
      },
      "source": [
        "<h2 id=\"importing_libraries\">Install and import libraries</h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install matplotlib --upgrade\n",
        "%pip install fancyimpute\n",
        "%pip install boruta\n",
        "%pip install imblearn\n",
        "%pip install xgboost\n",
        "%pip install \"numpy<1.24.0\"\n",
        "%pip install missingno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install sklearn --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd1W434rKgKE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import missingno as msno\n",
        "\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "\n",
        "# Import Scaler (normalizer)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Import Missing value imputers\n",
        "from sklearn.impute import KNNImputer\n",
        "from fancyimpute import IterativeImputer\n",
        "\n",
        "# Import Feature selection methods\n",
        "from boruta import BorutaPy\n",
        "from sklearn.feature_selection import RFE, SelectFromModel\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Import balancing methods\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Import models\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Import Pipeline\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Grid Search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Import model performance metrics\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, roc_curve, fbeta_score\n",
        "\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOjFojgdKgKF"
      },
      "source": [
        "# Data Understanding / Descriptive Analysis\n",
        "1. Histogram of percentage of missing values of features\n",
        "2. Histogram of volatilites of features\n",
        "3. Frequency distribution of target values\n",
        "4. Correlation heatmap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJGSjtepKgKG"
      },
      "source": [
        "# Manufacturing Operation Data (a.k.a Feature Data/Sensor Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "lqxi0ZdMKgKG",
        "outputId": "d0dfbab3-071d-44cd-f20e-f2d129f2f6f3"
      },
      "outputs": [],
      "source": [
        "# Read Manufacturing Operation Data (Feature Data/Sensor Data)\n",
        "sensor_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data\",sep=\" \", header=None)\n",
        "sensor_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQa_tRrFKgKH",
        "outputId": "702aa9b0-4349-4315-a01e-189fd282b99f"
      },
      "outputs": [],
      "source": [
        "# Data types in Feature Data\n",
        "type_dct_features = {str(k): len(list(v)) for k, v in sensor_data.groupby(sensor_data.dtypes, axis=1)}\n",
        "type_dct_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pVWCCmVKgKI"
      },
      "outputs": [],
      "source": [
        "# Add prefix \"feature\" to each column\n",
        "sensor_data = sensor_data.add_prefix(\"feature\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSXujXYpKgKI"
      },
      "source": [
        "# Descriptive analysis of whole data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu70UOIxKgKI"
      },
      "outputs": [],
      "source": [
        "# create dataframe for descriptive analysis\n",
        "descriptive_sensor = sensor_data.describe().transpose()\n",
        "\n",
        "# add column for number of unique values of each column\n",
        "descriptive_sensor[\"unique\"] = sensor_data.nunique()\n",
        "\n",
        "# add column for percentage of missing values of each column\n",
        "descriptive_sensor[\"missing_percentage\"] = sensor_data.isnull().sum() * 100 / len(sensor_data)\n",
        "\n",
        "# Define outliers based on Z-score\n",
        "def outliers_z_score(df,n):\n",
        "    outliers_list = []\n",
        "    threshold = n\n",
        "\n",
        "    for i in df.columns:\n",
        "        ys = df[i]\n",
        "        try:\n",
        "            mean_y = np.mean(ys)\n",
        "            stdev_y = np.std(ys)\n",
        "            z_scores = [(y - mean_y) / stdev_y for y in ys]\n",
        "            idx_outliers = np.where(np.abs(z_scores) > threshold)\n",
        "            outliers_list.append(len(idx_outliers[0]))\n",
        "        except:\n",
        "            outliers_list.append(np.NAN)\n",
        "    return outliers_list\n",
        "\n",
        "# add column for number of outliers of each column\n",
        "outlierls3s = outliers_z_score(sensor_data,3)\n",
        "descriptive_sensor[\"outliers(3s)\"] = outlierls3s\n",
        "\n",
        "outlierls4s = outliers_z_score(sensor_data,4)\n",
        "descriptive_sensor[\"outliers(4s)\"] = outlierls4s\n",
        "\n",
        "# add column for variance of each column\n",
        "descriptive_sensor[\"coeff_var\"] = descriptive_sensor[\"std\"]/np.absolute(descriptive_sensor[\"mean\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "rvVOtr0dKgKJ",
        "outputId": "ca570d29-3083-478f-ff4b-8303d590ff40"
      },
      "outputs": [],
      "source": [
        "descriptive_sensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLrBygYjKgKJ"
      },
      "source": [
        "# Semiconductor Quality Data (a.k.a Target Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "P8Vms3g1KgKK",
        "outputId": "f00cf576-1c69-4e9c-bd04-095a1300a4d3"
      },
      "outputs": [],
      "source": [
        "# Read semiconductor quality data (target)\n",
        "target_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data\",sep=\" \",header=None)\n",
        "target_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsTgpCPuKgKK",
        "outputId": "bec64fad-21e9-45b9-eb00-f4194a1e2406"
      },
      "outputs": [],
      "source": [
        "# Data types in Label Data\n",
        "type_dct = {str(k): len(list(v)) for k, v in target_data.groupby(target_data.dtypes, axis=1)}\n",
        "type_dct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOG4iXRSKgKK"
      },
      "outputs": [],
      "source": [
        "# Change column names\n",
        "target_data.columns = [\"Label\",\"Time\"]\n",
        "\n",
        "# Convert type of columns\n",
        "target_data[\"Label\"] = target_data[\"Label\"].astype(\"category\")\n",
        "\n",
        "# Convert format of Time Column as datetime\n",
        "target_data[\"Time\"] = pd.to_datetime(target_data[\"Time\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJrMjUuVKgKL"
      },
      "source": [
        "## Distribution of Target Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 858
        },
        "id": "HJXMCcoWKgKL",
        "outputId": "a1fb8c49-b12d-4829-b373-2c749b75a9a1"
      },
      "outputs": [],
      "source": [
        "# Set size of chart\n",
        "plt.figure(figsize = (10,10))\n",
        "\n",
        "# Labels for data\n",
        "keys = ['Pass','Fail']\n",
        "\n",
        "# Plotting data on Pie chart\n",
        "Piechart_Labels = plt.pie(target_data.Label.value_counts(), labels=keys, autopct='%.2f%%', textprops={'fontsize': 20})\n",
        "\n",
        "# Add title to the chart\n",
        "plt.title('Distribution of Target Labels (Whole Data)',fontdict={'size':24})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3XtUnY5KgKM"
      },
      "source": [
        "## Timeseries of Target Label Frequencies (Pass/Fail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTs-cMSUKgKM",
        "outputId": "a17f8f21-c291-481b-d21b-4f2dd186f172"
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "\n",
        "# Create a Date column from Time (timestamp) Column of Label Data\n",
        "target_data[\"Date\"] = target_data[\"Time\"].dt.date\n",
        "\n",
        "# check first and last dates of Label Data\n",
        "print(\"first date = {}\".format(target_data[\"Date\"].min()))\n",
        "print(\"last date = {}\".format(target_data[\"Date\"].max()))\n",
        "\n",
        "# Create a Dataframe by Grouping Labels by Date and calculating the frequencies (count) of Label Values\n",
        "timeseries_label_count = pd.DataFrame(target_data.groupby(\"Date\")[\"Label\"].value_counts())\n",
        "\n",
        "# Rename Calculated column as \"Count\"\n",
        "timeseries_label_count = timeseries_label_count.rename(columns={'Label': 'Count'})\n",
        "\n",
        "# Reset index of grouped Dataframe\n",
        "timeseries_label_count.reset_index(inplace=True)\n",
        "\n",
        "# Rename Label values as Pass and Fail\n",
        "timeseries_label_count[\"Label\"] = timeseries_label_count[\"level_1\"].replace(to_replace=[-1, 1], value=[\"Pass\",\"Fail\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "KZSMSu0zKgKM",
        "outputId": "85fc5d13-12b3-484f-bbfc-ea06f538471d"
      },
      "outputs": [],
      "source": [
        "# Set size of chart\n",
        "plt.figure(figsize = (13,8))\n",
        "\n",
        "# create the scatter plot\n",
        "timeseries_label_scatterplot = sns.scatterplot(data=timeseries_label_count, x=\"Date\", y=\"Count\", size = \"Count\", hue=\"Label\", sizes=(20, 200))\n",
        "\n",
        "timeseries_label_scatterplot.set_title('Target Frequencies by Time', fontdict={'size':24})\n",
        "timeseries_label_scatterplot.set_xlabel('Date',fontdict={'size':15})\n",
        "timeseries_label_scatterplot.set_ylabel('Frequency', fontdict={'size':15})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBKN1iGgKgKN"
      },
      "source": [
        "# Create train and test dataset\n",
        "\n",
        "1. First we merge the data\n",
        "2. Then we drop the Date and Time columns since we don't need them anymore.\n",
        "3. According to dataset's description, target values are highly imbalanced, so we split it in a stratified fashion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hvul2LWmKgKN"
      },
      "outputs": [],
      "source": [
        "# Merge sensor and label data\n",
        "merged_df = pd.concat([target_data,sensor_data],axis=1)\n",
        "merged_df.drop([\"Date\",\"Time\"], axis=1, inplace=True)\n",
        "\n",
        "# Convert labels into text categories\n",
        "merged_df[\"Label\"] = merged_df[\"Label\"].replace({-1:\"PASS\", 1:\"FAIL\"})\n",
        "\n",
        "# Create training and test datasets\n",
        "X = merged_df.drop([\"Label\"],axis=1)\n",
        "Y = merged_df[\"Label\"]\n",
        "\n",
        "\n",
        "# Split data into train and test by 80%-20% in a stratified fashion\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42, stratify=Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "xww7_zyNKgKN",
        "outputId": "6ffadfc3-c5ed-47a0-e91f-1af585da2aff"
      },
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpZhRueAKgKO"
      },
      "source": [
        "# Descriptive Statistics of Target Train/Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 858
        },
        "id": "yzaJGEeTKgKO",
        "outputId": "e33a8a93-6b63-4793-c97b-0c200c58e206"
      },
      "outputs": [],
      "source": [
        "# Set size of chart\n",
        "plt.figure(figsize = (10,10))\n",
        "\n",
        "# Labels for data\n",
        "keys = ['Pass','Fail']\n",
        "\n",
        "# Plotting data on Pie chart\n",
        "Piechart_Labels_train = plt.pie(Y_train.value_counts(), labels=keys, autopct='%.2f%%', textprops={'fontsize': 20})\n",
        "\n",
        "# Add title to the chart\n",
        "plt.title('Distribution of Target Labels (Train Set)',fontdict={'size':24})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 858
        },
        "id": "gQ1rIYXeKgKO",
        "outputId": "277c063e-ab59-4fe2-d2d6-4e49c883d16d"
      },
      "outputs": [],
      "source": [
        "# Set size of chart\n",
        "plt.figure(figsize = (10,10))\n",
        "\n",
        "# Labels for data\n",
        "keys = ['Pass','Fail']\n",
        "\n",
        "# Plotting data on Pie chart\n",
        "Piechart_Labels_train = plt.pie(Y_test.value_counts(), labels=keys, autopct='%.2f%%', textprops={'fontsize': 20})\n",
        "\n",
        "# Add title to the chart\n",
        "plt.title('Distribution of Target Labels (Test Set)',fontdict={'size':24})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD25LcPGKgKP"
      },
      "source": [
        "# Descriptive Statistics of Feature Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "UIx0jv5QKgKP",
        "outputId": "926a8301-8178-4020-e003-ddcff801733c"
      },
      "outputs": [],
      "source": [
        "print(\"shape of feature train set :{} and shape of feature test set: {}\".format(X_train.shape, X_test.shape))\n",
        "print(\"shape of target train set :{} and shape of target test set: {}\".format(Y_train.shape, Y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzc1JkJMKgKP"
      },
      "source": [
        "### > <font color='green'>Descriptive Analysis of X_train</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sax2dTtOKgKQ"
      },
      "outputs": [],
      "source": [
        "# create dataframe for descriptive analysis\n",
        "descriptive_train = X_train.describe().transpose()\n",
        "\n",
        "# add column for number of unique values of each column\n",
        "descriptive_train[\"unique\"] = X_train.nunique()\n",
        "\n",
        "# add column for percentage of missing values of each column\n",
        "descriptive_train[\"missing_percentage\"] = X_train.isnull().sum() * 100 / len(X_train)\n",
        "\n",
        "# add column for number of outliers of each column\n",
        "outlierls3s_ = outliers_z_score(X_train,3)\n",
        "descriptive_train[\"outliers(3s)\"] = outlierls3s_\n",
        "\n",
        "outlierls4s_ = outliers_z_score(X_train,4)\n",
        "descriptive_train[\"outliers(4s)\"] = outlierls4s_\n",
        "\n",
        "# add column for coefficient of variance of each column\n",
        "descriptive_train[\"coeff_var\"] = descriptive_train[\"std\"]/np.absolute(descriptive_train[\"mean\"])\n",
        "descriptive_train[\"coeff_var\"] = descriptive_train[\"coeff_var\"].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "8xA1S3VhwX9i",
        "outputId": "183774eb-144c-4a9e-acb4-99b4858d3812"
      },
      "outputs": [],
      "source": [
        "descriptive_train[descriptive_train[\"coeff_var\"]<=0.25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "AVb_kWhSKgKQ",
        "outputId": "84ea5a9a-951e-4ccd-8f35-2ee026c2d39e"
      },
      "outputs": [],
      "source": [
        "descriptive_train[descriptive_train[\"unique\"]==1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn922TKjKgKQ"
      },
      "source": [
        "## 1. Histogram of Missing Values of Feature Train Set (Percentage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "id": "TFTOWZvaKgKQ",
        "outputId": "708071cf-7735-4d4c-9f97-23447aca1909"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,8))\n",
        "\n",
        "missingval_chart = sns.histplot(descriptive_train, x=\"missing_percentage\", binwidth=5, stat='count',legend=True)\n",
        "missingval_chart.set_title('Percentage of Missing Values of Feature Train Set (y-axis capped at 25)', fontdict={'size':24})\n",
        "missingval_chart.set_xlabel('Missing Values (%)',fontdict={'size':15})\n",
        "missingval_chart.set_ylabel('Frequency', fontdict={'size':15})\n",
        "missingval_chart.set_xticks(range(0,100,5))\n",
        "\n",
        "for c in missingval_chart.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    missingval_chart.bar_label(c, labels=labels, fontsize=12, padding=3,label_type='center')\n",
        "\n",
        "plt.ylim(0, 25)\n",
        "plt.xlim(0,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r_TO5YUKgKR",
        "outputId": "4203027e-4901-4ca6-a57d-21942516dfc5"
      },
      "outputs": [],
      "source": [
        "# Number of Features having 50% or more missing values\n",
        "missing_50 = descriptive_train[descriptive_train[\"missing_percentage\"]>=50]\n",
        "missing_50_cols = missing_50.index\n",
        "len(missing_50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgR_tKM1KgKR",
        "outputId": "cf75141f-5dfb-4ef9-d44c-331128eac18e"
      },
      "outputs": [],
      "source": [
        "missing_50_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSn13n6MKgK2"
      },
      "source": [
        "## 2. Histogram of Volatilites of Feature Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "k1KZY9PHKgK2",
        "outputId": "5b1de89f-98d2-45c2-bd71-c16d6b9e49c9"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,8))\n",
        "volatilities_chart1 = sns.histplot(descriptive_train, x=\"coeff_var\", kde=True, binwidth=0.25)\n",
        "volatilities_chart1.set_title('Volatilites of Feature Train Set (x-axis capped at 10)', fontdict={'size':24})\n",
        "volatilities_chart1.set_xlabel('Coefficient of Variance',fontdict={'size':15})\n",
        "volatilities_chart1.set_ylabel('Frequency', fontdict={'size':15})\n",
        "volatilities_chart1.set_xticks(range(0,10,1))\n",
        "\n",
        "for c in volatilities_chart1.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    volatilities_chart1.bar_label(c, labels=labels, fontsize=12, padding=3)\n",
        "\n",
        "plt.xlim(0,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "4NWXETk-KgK3",
        "outputId": "f8d8905a-a872-4415-965b-0dd4c2e8cdac"
      },
      "outputs": [],
      "source": [
        "# Select features having 0.25 or less coefficient of variance\n",
        "coeff_variance_lessthan25percent = descriptive_train[descriptive_train[\"coeff_var\"]<=0.25]\n",
        "coeff_variance_lessthan25percent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "EBbf35lsKgK3",
        "outputId": "8a6fcf3b-7bed-46b6-c6de-c1d253c8cb54"
      },
      "outputs": [],
      "source": [
        "# Plot histogram of features having 0.01 or less coefficient of variance\n",
        "plt.figure(figsize = (15,8))\n",
        "volatilities_chart2 = sns.histplot(coeff_variance_lessthan25percent, x=\"coeff_var\", kde=True, binwidth=0.01)\n",
        "volatilities_chart2.set_title('Volatilities of Feature Train Set', fontdict={'size':24})\n",
        "volatilities_chart2.set_xlabel('Coefficient of Variance',fontdict={'size':15})\n",
        "volatilities_chart2.set_ylabel('Frequency', fontdict={'size':15})\n",
        "\n",
        "for c in volatilities_chart2.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    volatilities_chart2.bar_label(c, labels=labels, fontsize=12, padding=3)\n",
        "\n",
        "plt.ylim(0, 25)\n",
        "plt.xlim(0,0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gu6e9RruKgK3"
      },
      "source": [
        "## 3. Constant Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-o0vloTKgK4",
        "outputId": "1ad194c7-5c7e-45f5-ca0a-54e8f8d83bfe"
      },
      "outputs": [],
      "source": [
        "# Select features having zero coefficient of variance\n",
        "constant_columns = descriptive_train[descriptive_train[\"coeff_var\"]==0].index\n",
        "constant_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKMcWxz8KgK4",
        "outputId": "3ff23cae-da44-4041-ef22-32b384f52f92"
      },
      "outputs": [],
      "source": [
        "# Select features having zero coefficient of variance\n",
        "constant_columns = descriptive_train[descriptive_train[\"std\"]==0].index\n",
        "len(constant_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s1XlE8wKgK4",
        "outputId": "6694cc60-2ae4-4816-b8e4-c5d33ebed3ba"
      },
      "outputs": [],
      "source": [
        "# Select features having zero coefficient of variance\n",
        "constant_columns = descriptive_train[descriptive_train[\"unique\"]==1].index\n",
        "len(constant_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWjXTyCQKgK4"
      },
      "source": [
        "## 4. Duplicated Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9c1onXqKgK5",
        "outputId": "2687ff71-9a82-4fa6-9195-ad88b7ba50eb"
      },
      "outputs": [],
      "source": [
        "# Create Dataframe for duplicated columns in feature dataset(True/False)\n",
        "duplicated_df = pd.DataFrame(X_train.transpose().duplicated())\n",
        "\n",
        "# Change column name\n",
        "duplicated_df.columns = [\"duplicated\"]\n",
        "\n",
        "# Get only True values for duplicated columns\n",
        "duplicated_columns = duplicated_df[duplicated_df[\"duplicated\"]==True].index\n",
        "\n",
        "print(\"Number of duplicated columns = {}\".format(len(duplicated_columns)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "jw529mS8KgK5",
        "outputId": "37180056-b96c-4e03-cac2-0a2d362a1b90"
      },
      "outputs": [],
      "source": [
        "X_train[duplicated_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye1d1JyHKgK5",
        "outputId": "784c7e9f-af57-4cda-91a0-21dd66a1c00a"
      },
      "outputs": [],
      "source": [
        "# Check how much of duplicated features are contained in constant features\n",
        "duplicated_columns.isin(constant_columns).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRFcUOKfKgK6"
      },
      "source": [
        "## 5. Histogram of Number of Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S14O8g02KgK6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,8))\n",
        "outliers_chart3s = sns.histplot(descriptive_train, x=\"outliers(3s)\", binwidth=5)\n",
        "outliers_chart3s.set_title('Histogram of Outliers of Feature Train Set (3s Rule)', fontdict={'size':24})\n",
        "outliers_chart3s.set_xlabel('Number of Outliers',fontdict={'size':15})\n",
        "outliers_chart3s.set_ylabel('Frequency', fontdict={'size':15})\n",
        "for c in outliers_chart3s.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    outliers_chart3s.bar_label(c, labels=labels, fontsize=12, padding=3,label_type='center')\n",
        "plt.xlim(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "id": "L_6bMvHyKgK6",
        "outputId": "ab0d3a41-e404-4ddc-b2eb-6c5898100f35"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,8))\n",
        "outliers_chart4s = sns.histplot(descriptive_train, x=\"outliers(4s)\", binwidth=5)\n",
        "outliers_chart4s.set_title('Outliers of Feature Train Set Before Treatment (4s Rule)', fontdict={'size':24})\n",
        "outliers_chart4s.set_xlabel('Number of Outliers',fontdict={'size':15})\n",
        "outliers_chart4s.set_ylabel('Frequency', fontdict={'size':15})\n",
        "\n",
        "for c in outliers_chart4s.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    outliers_chart4s.bar_label(c, labels=labels, fontsize=12, padding=3,label_type='center')\n",
        "\n",
        "plt.ylim(0,110)\n",
        "\n",
        "plt.xlim(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Wlwn2kCKgK8"
      },
      "source": [
        "## 5. Correlation Heatmap of Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKsw3UJCKgK8"
      },
      "source": [
        "## Drop constant features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN5wC6OHKgK8",
        "outputId": "3c494b36-092c-4d89-855b-4fe731f33d24"
      },
      "outputs": [],
      "source": [
        "# Drop constant features by excluding them from train set\n",
        "constant_columns_list = list(constant_columns)\n",
        "constants_dropped = X_train.drop(constant_columns_list,axis=1)\n",
        "print(\"{} columns were dropped\".format(X_train.shape[1] - constants_dropped.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "df1_C-iYKgK9",
        "outputId": "58be6e27-0dd8-40d6-8094-0aebc55f2ee3"
      },
      "outputs": [],
      "source": [
        "corr = constants_dropped.corr()\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize = (20,8))\n",
        "correlation_heatmap_constants_dropped = sns.heatmap(corr)\n",
        "correlation_heatmap_constants_dropped.set_title('Correlation Heatmap of Features', fontdict={'size':24})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht0C_HkpKgK9"
      },
      "source": [
        "# 6. Feature Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9StHVNUHKgK9"
      },
      "outputs": [],
      "source": [
        "# Remove the features with more than 50% of Missing Values\n",
        "missing_perc = pd.DataFrame(constants_dropped.isnull().sum()/len(constants_dropped)*100)\n",
        "missing_perc.columns = [\"percentage\"]\n",
        "missing_50_col_list = missing_perc[missing_perc[\"percentage\"]>50].index\n",
        "\n",
        "missing_50_col_list = list(missing_50_cols)\n",
        "X_train = constants_dropped.drop(missing_50_col_list,axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bd891VWKgK-",
        "outputId": "ae5563f0-dd53-466f-e40b-4dea36135fa8"
      },
      "outputs": [],
      "source": [
        "len(missing_50_col_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlZfjYfUKgK-"
      },
      "outputs": [],
      "source": [
        "# Function to control number of remaining features in every step\n",
        "def remainingFeatures(df):\n",
        "    name = [name for name, value in globals().items() if value is df][0]\n",
        "    # To visualize dataframe name in printing\n",
        "    print(\"Remaining Features of \"+str(name)+\": \"+str(df.shape[1]))\n",
        "\n",
        "# Function to control number of remaining nan´s in every step\n",
        "def naCounter(df):\n",
        "    # Count total NaN´s in a Dataframe\n",
        "    na_count = df.isna().sum().sum()\n",
        "    # To visualize dataframe name in printing\n",
        "    name = [name for name, value in globals().items() if value is df][0]\n",
        "\n",
        "    # Print total Nan´s\n",
        "    print(\"Total NaN of \"+str(name)+\": \"+str(na_count))\n",
        "\n",
        "    return na_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csW7eAOfKgK-",
        "outputId": "51a16f5b-478a-40a3-c24e-46d58f32a025"
      },
      "outputs": [],
      "source": [
        "remainingFeatures(X_train)\n",
        "naCounter(X_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va0oinmPKgK_"
      },
      "source": [
        "### > <font color='green'>Descriptive Analysis of X_train after feature removal </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj28H0UEKgK_"
      },
      "outputs": [],
      "source": [
        "# create dataframe for descriptive analysis\n",
        "descriptive_train_after_removal = X_train.describe().transpose()\n",
        "\n",
        "# add column for number of unique values of each column\n",
        "descriptive_train_after_removal[\"unique\"] = X_train.nunique()\n",
        "\n",
        "# add column for percentage of missing values of each column\n",
        "descriptive_train_after_removal[\"missing_percentage\"] = X_train.isnull().sum() * 100 / len(X_train)\n",
        "\n",
        "# add column for number of outliers of each column\n",
        "outlierls3s_ = outliers_z_score(X_train,3)\n",
        "descriptive_train_after_removal[\"outliers(3s)\"] = outlierls3s_\n",
        "\n",
        "outlierls4s_ = outliers_z_score(X_train,4)\n",
        "descriptive_train_after_removal[\"outliers(4s)\"] = outlierls4s_\n",
        "\n",
        "# add column for coefficient of variance of each column\n",
        "descriptive_train_after_removal[\"coeff_var\"] = descriptive_train_after_removal[\"std\"]/np.absolute(descriptive_train_after_removal[\"mean\"])\n",
        "descriptive_train_after_removal[\"coeff_var\"] = descriptive_train_after_removal[\"coeff_var\"].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "7DulF8ceKgK_",
        "outputId": "b49d64a8-3a93-4c24-bf87-6dcd72482136"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,8))\n",
        "volatilities_chart1 = sns.histplot(descriptive_train, x=\"coeff_var\", kde=True, binwidth=0.25)\n",
        "volatilities_chart1.set_title('Volatilites of Feature Train Set Before Dimensionality Reduction (x-axis capped at 10)', fontdict={'size':24})\n",
        "volatilities_chart1.set_xlabel('Coefficient of Variance',fontdict={'size':15})\n",
        "volatilities_chart1.set_ylabel('Frequency', fontdict={'size':15})\n",
        "volatilities_chart1.set_xticks(range(0,10,1))\n",
        "\n",
        "for c in volatilities_chart1.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    volatilities_chart1.bar_label(c, labels=labels, fontsize=12, padding=3)\n",
        "\n",
        "plt.xlim(0,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "wMG1qvVwKgK_",
        "outputId": "d526bae0-026a-430d-de6d-76b5847132b8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,8))\n",
        "volatilities_chart1 = sns.histplot(descriptive_train_after_removal, x=\"coeff_var\", kde=True, binwidth=0.25)\n",
        "volatilities_chart1.set_title('Volatilites of Feature Train Set After Dimensionality Reduction (x-axis capped at 10)', fontdict={'size':24})\n",
        "volatilities_chart1.set_xlabel('Coefficient of Variance',fontdict={'size':15})\n",
        "volatilities_chart1.set_ylabel('Frequency', fontdict={'size':15})\n",
        "volatilities_chart1.set_xticks(range(0,10,1))\n",
        "\n",
        "for c in volatilities_chart1.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    volatilities_chart1.bar_label(c, labels=labels, fontsize=12, padding=3)\n",
        "\n",
        "plt.xlim(0,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "nkxacdMvKgLA",
        "outputId": "4fc8b266-b82a-49c1-c5a0-2222222e26d6"
      },
      "outputs": [],
      "source": [
        "descriptive_train[descriptive_train[\"coeff_var\"]<=0.25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "7tat_L76KgLA",
        "outputId": "c968460b-79d2-4f97-f907-b68e810a990f"
      },
      "outputs": [],
      "source": [
        "# Select features having 0.25 or less coefficient of variance\n",
        "coeff_variance_lessthan25percent_after_removal = descriptive_train_after_removal[descriptive_train_after_removal[\"coeff_var\"]<=0.25]\n",
        "coeff_variance_lessthan25percent_after_removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "7omm_EyuKgLA",
        "outputId": "141cd2df-3250-4b12-9fdf-7212e96ae2ee"
      },
      "outputs": [],
      "source": [
        "# Plot histogram of features having 0.01 or less coefficient of variance\n",
        "plt.figure(figsize = (15,8))\n",
        "volatilities_chart2 = sns.histplot(coeff_variance_lessthan25percent_after_removal, x=\"coeff_var\", kde=True, binwidth=0.01)\n",
        "volatilities_chart2.set_title('Volatilities of Feature Train Set After Dimensionality Reduction', fontdict={'size':24})\n",
        "volatilities_chart2.set_xlabel('Coefficient of Variance',fontdict={'size':15})\n",
        "volatilities_chart2.set_ylabel('Frequency', fontdict={'size':15})\n",
        "\n",
        "for c in volatilities_chart2.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    volatilities_chart2.bar_label(c, labels=labels, fontsize=12, padding=3)\n",
        "\n",
        "plt.xlim(0,0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8JKObtPKgLB"
      },
      "outputs": [],
      "source": [
        "# histogram of some of the features\n",
        "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
        "    %matplotlib inline\n",
        "    plt.rcParams.update({'figure.figsize':(8,8), 'figure.dpi':100})\n",
        "\n",
        "\n",
        "    nunique = df.nunique()\n",
        "    df = df[nunique[(nunique>1)&(nunique<50)].index] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
        "    nRow, nCol = df.shape\n",
        "    columnNames = list(df)\n",
        "    nGraphRow = int((nCol + nGraphPerRow - 1) / nGraphPerRow)\n",
        "    %matplotlib inline\n",
        "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
        "\n",
        "\n",
        "    for i in range(min(nCol, nGraphShown)):\n",
        "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
        "        columnDf = df.iloc[:, i]\n",
        "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
        "            valueCounts = columnDf.value_counts()\n",
        "            valueCounts.plot.bar()\n",
        "        else:\n",
        "            columnDf.hist()\n",
        "        plt.ylabel('counts')\n",
        "        plt.xticks(rotation = 90)\n",
        "        plt.title(f'{columnNames[i]} (column {i})')\n",
        "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxNKMBoOKgLB"
      },
      "source": [
        "# 7. Outlier Treatment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKMIGzf_KgLB"
      },
      "outputs": [],
      "source": [
        "# We define a function to count number of outliers\n",
        "def outlierCounter(df,number_of_std):\n",
        "    lower_limit = df.mean() - number_of_std * df.std()\n",
        "    upper_limit = df.mean() + number_of_std * df.std()\n",
        "\n",
        "    # Identify outliers using the limits defined by std number\n",
        "    outliers = (df < lower_limit) | (df > upper_limit)\n",
        "\n",
        "    # Here we count the total of \"Trues\" and \"Falses\"\n",
        "    true_count = outliers.values.sum()\n",
        "    false_count = np.logical_not(outliers.values).sum()\n",
        "\n",
        "    # To visualize dataframe name in printing\n",
        "    name = [name for name, value in globals().items() if value is df][0]\n",
        "\n",
        "    # Imprimir los resultados\n",
        "    print(\"Total Outliers Data Points of \"+str(name)+\": \"+str(true_count))\n",
        "    print(\"Total Data Points of \"+str(name)+\": \"+str(false_count+true_count))\n",
        "    print(\"Total Data Points of \"+str(name)+\": \" +str(true_count/(false_count+true_count)))\n",
        "\n",
        "    return outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q95rqopNKgLB",
        "outputId": "24f9b113-bd6f-46e7-fb67-4cd40176bb3d"
      },
      "outputs": [],
      "source": [
        "outlierCounter(X_train,4)\n",
        "naCounter(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3vdm8LgKgLC"
      },
      "outputs": [],
      "source": [
        "def outliers_treatment(df,number_of_std):\n",
        "    # Calculate inferior and superior limit following the n*s rules selected\n",
        "    inf_limit = df.mean() - number_of_std * df.std()\n",
        "    sup_limit = df.mean() + number_of_std * df.std()\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    for col in df.columns:\n",
        "        #df[col] = df[col].fillna(df[col].median())\n",
        "        # Identifying outliers using n*s rule selected\n",
        "        outliers = (df[col] < inf_limit[col]) | (df[col] > sup_limit[col])\n",
        "        # Imputation of outliers using the superior limit\n",
        "        #df[col] = np.where(outliers, sup_limit[col], df[col])\n",
        "        df.loc[outliers, col] = np.nan\n",
        "        # This is an alternative if we want to use the median as replacement\n",
        "        #df[col] = np.where(outliers, df[col].median(), df[col])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okXlaBzsKgLC"
      },
      "source": [
        "### > <font color='green'>outliers treated X_train</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDCkcr6XKgLC"
      },
      "outputs": [],
      "source": [
        "X_train_4s = outliers_treatment(X_train,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1j8s6kOKgLC",
        "outputId": "113fa8d0-79c3-4c1e-fb1a-05798055850f"
      },
      "outputs": [],
      "source": [
        "outliers_before_treatment = outlierCounter(X_train,4)\n",
        "outliers_after_treatment = outlierCounter(X_train_4s,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efrGYREFKgLD",
        "outputId": "5eadc535-618e-4df6-d04a-3eb64aba66f7"
      },
      "outputs": [],
      "source": [
        "na_before_treatment = naCounter(X_train)\n",
        "na_after_treatment = naCounter(X_train_4s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0arkMU9KgLE",
        "outputId": "0438b2ae-811f-4cdb-e740-9d2533538dee"
      },
      "outputs": [],
      "source": [
        "na_percentage_before_treatment = na_before_treatment/(X_train.shape[0]*X_train.shape[1])*100\n",
        "na_percentage_before_treatment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD3Mv0mVKgLF",
        "outputId": "4bc99a8d-1583-4098-d314-ab0e16e3d17a"
      },
      "outputs": [],
      "source": [
        "na_percentage_after_treatment = na_after_treatment/(X_train_4s.shape[0]*X_train_4s.shape[1])*100\n",
        "na_percentage_after_treatment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1nAEliUKgLG"
      },
      "source": [
        "### > <font color='green'>Descriptive Analysis of imputed X_train</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJafGnUiKgLH"
      },
      "outputs": [],
      "source": [
        "# create dataframe for descriptive analysis\n",
        "descriptive_X_train_4s = X_train_4s.describe().transpose()\n",
        "\n",
        "# add column for percentage of missing values of each column\n",
        "descriptive_X_train_4s[\"missing_percentage\"] = X_train_4s.isnull().sum() * 100 / len(X_train_4s)\n",
        "\n",
        "# add column for number of unique values of each column\n",
        "descriptive_X_train_4s[\"unique\"] = X_train_4s.nunique()\n",
        "\n",
        "# add column for outliers\n",
        "outlierls_train_4s = outliers_z_score(X_train_4s,4)\n",
        "descriptive_X_train_4s[\"outliers(4s)\"] = outlierls_train_4s\n",
        "\n",
        "# add column for variance of each column\n",
        "descriptive_X_train_4s[\"coeff_var\"] = descriptive_X_train_4s[\"std\"]/np.absolute(descriptive_X_train_4s[\"mean\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "pA9X7UvmKgLH",
        "outputId": "b51c0c0b-3906-451f-deaf-135eb888c669"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,8))\n",
        "outliers_chart4s = sns.histplot(descriptive_train_after_removal, x=\"outliers(4s)\", binwidth=5)\n",
        "outliers_chart4s.set_title('Outliers of Feature Train Set Before Treatment (4s Rule)', fontdict={'size':24})\n",
        "outliers_chart4s.set_xlabel('Number of Outliers',fontdict={'size':15})\n",
        "outliers_chart4s.set_ylabel('Frequency', fontdict={'size':15})\n",
        "\n",
        "for c in outliers_chart4s.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    outliers_chart4s.bar_label(c, labels=labels, fontsize=12, padding=3)\n",
        "\n",
        "plt.xlim(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "-7AhbEtyKgLH",
        "outputId": "7513fd48-54eb-416e-cb77-e1e1a7973920"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,8))\n",
        "outliers_chart_X_train_4s = sns.histplot(descriptive_X_train_4s, x=\"outliers(4s)\", binwidth=5)\n",
        "outliers_chart_X_train_4s.set_title('Outliers of Feature Train Set After Treatment (4s Rule)', fontdict={'size':24})\n",
        "outliers_chart_X_train_4s.set_xlabel('Number of Outliers',fontdict={'size':15})\n",
        "outliers_chart_X_train_4s.set_ylabel('Frequency', fontdict={'size':15})\n",
        "\n",
        "for c in outliers_chart_X_train_4s.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    outliers_chart_X_train_4s.bar_label(c, labels=labels, fontsize=12, padding=3)\n",
        "\n",
        "\n",
        "plt.xlim(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "xNieZFdwKgLI",
        "outputId": "224103c9-b06f-4f5a-cebf-c01f93b1fed4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,8))\n",
        "\n",
        "missingval_chart = sns.histplot(descriptive_train_after_removal, x=\"missing_percentage\", binwidth=5, stat='count',legend=True)\n",
        "missingval_chart.set_title('Percentage of Missing Values of Features Train Set Before Outlier Treatment (y-axis capped at 25)', fontdict={'size':24})\n",
        "missingval_chart.set_xlabel('Missing Values (%)',fontdict={'size':15})\n",
        "missingval_chart.set_ylabel('Frequency', fontdict={'size':15})\n",
        "missingval_chart.set_xticks(range(0,100,5))\n",
        "\n",
        "for c in missingval_chart.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    missingval_chart.bar_label(c, labels=labels, fontsize=12, padding=3)\n",
        "\n",
        "plt.xlim(0,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "qwkdUhoLKgLI",
        "outputId": "e68c2315-f0a0-4aff-9482-76b96f47e925"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,8))\n",
        "\n",
        "missingval_chart_X_train_4s = sns.histplot(descriptive_X_train_4s, x=\"missing_percentage\", binwidth=5, stat='count',legend=True)\n",
        "missingval_chart_X_train_4s.set_title('Percentage of Missing Values of Features Train Set After Outlier Treatment (y-axis capped at 25)', fontdict={'size':24})\n",
        "missingval_chart_X_train_4s.set_xlabel('Missing Values (%)',fontdict={'size':15})\n",
        "missingval_chart_X_train_4s.set_ylabel('Frequency', fontdict={'size':15})\n",
        "missingval_chart_X_train_4s.set_xticks(range(0,100,5))\n",
        "\n",
        "for c in missingval_chart_X_train_4s.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    missingval_chart_X_train_4s.bar_label(c, labels=labels, fontsize=12, padding=3)\n",
        "\n",
        "plt.xlim(0,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "auFpOShuKgLJ",
        "outputId": "f3bfd771-04f4-428c-ba64-73b4f084f09f"
      },
      "outputs": [],
      "source": [
        "X_train_4s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw2lxG3GKgLJ"
      },
      "source": [
        "# 8. Missing Values Treatment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz97u7uslQUf"
      },
      "source": [
        "## X_train_4s normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY0VvPntKgLM"
      },
      "outputs": [],
      "source": [
        "def normalizer(df, scaler):\n",
        "    scaler = scaler\n",
        "    scaler.fit(df)\n",
        "    scaled_df = pd.DataFrame(scaler.transform(df), index=df.index, columns=df.columns)\n",
        "\n",
        "    return scaled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "1OaZdvDCKgLM",
        "outputId": "de90ecaf-fbac-4df3-ac06-3fd629d524f2"
      },
      "outputs": [],
      "source": [
        "X_train_4s_normalized = normalizer(X_train_4s, MinMaxScaler())\n",
        "X_train_4s_normalized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkBhaMWcKgLN"
      },
      "outputs": [],
      "source": [
        "# create dataframe for descriptive analysis\n",
        "descriptive_normalized_X_train_4s = X_train_4s_normalized.describe().transpose()\n",
        "\n",
        "# add column for percentage of missing values of each column\n",
        "descriptive_normalized_X_train_4s[\"missing_percentage\"] = X_train_4s_normalized.isnull().sum() * 100 / len(X_train_4s_normalized)\n",
        "\n",
        "# add column for number of unique values of each column\n",
        "descriptive_normalized_X_train_4s[\"unique\"] = X_train_4s_normalized.nunique()\n",
        "\n",
        "# add column for outliers\n",
        "outlierls_normalized_X_train_4s = outliers_z_score(X_train_4s_normalized,4)\n",
        "descriptive_normalized_X_train_4s[\"outliers(4s)\"] = outlierls_normalized_X_train_4s\n",
        "\n",
        "# add column for variance of each column\n",
        "descriptive_normalized_X_train_4s[\"coeff_var\"] = descriptive_normalized_X_train_4s[\"std\"]/np.absolute(descriptive_normalized_X_train_4s[\"mean\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGyq0zUjKgLN"
      },
      "source": [
        "## 8.1.1 Imputation Method #1: HOTDECK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0MYeVJ9KgLN"
      },
      "outputs": [],
      "source": [
        "# We create a function to develop the Hotdeck imputation method\n",
        "def imputeHOTDECK(df):\n",
        "\n",
        "    # We copy the dataframe to preserve it\n",
        "    df_imputed = df.copy()\n",
        "\n",
        "    # We iterate over rows with missing values\n",
        "    for i, row in df_imputed.iterrows():\n",
        "        # Verify if there are na´s\n",
        "        if row.isnull().any():\n",
        "            # Calculate distances between current row and the other rows\n",
        "            distances = cdist(row.values.reshape(1, -1), df_imputed.drop(i).values)\n",
        "            # Find the closest row using Euclidean distance (standard option)\n",
        "            most_similar_row = np.argmin(distances)\n",
        "            # Imputate missing values with values from closest row values\n",
        "            df_imputed.loc[i, row.isnull()] = df_imputed.iloc[most_similar_row][row.isnull()]\n",
        "\n",
        "    return df_imputed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC-m1Hc2KgLP"
      },
      "source": [
        "## 8.1.2 Imputation Method #2: kNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va5BweWgKgLQ"
      },
      "outputs": [],
      "source": [
        "def imputeKNN(df, nn):\n",
        "    # We copy the dataframe to preserve it\n",
        "    df_imputed = df.copy()\n",
        "    # We create kNN object with \"nn\" neighbor number (5 as standard)\n",
        "    knn_imputer = KNNImputer(n_neighbors=nn)\n",
        "    # We made imputation fitting\n",
        "    df_imputed = knn_imputer.fit_transform(df)\n",
        "    # Turn into new dataframe\n",
        "    df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
        "    # Return imputed dataframe\n",
        "    df_imputed.index = df.index\n",
        "    return df_imputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BPEkv1IKgLR",
        "outputId": "6a50debe-d27b-408c-db27-9608c7c0db98"
      },
      "outputs": [],
      "source": [
        "X_train_4s_normalized_KNN = imputeKNN(X_train_4s_normalized, 5)\n",
        "naCounter(X_train_4s_normalized_KNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inverse_X_train_4s_normalized_KNN = pd.DataFrame(normalizer.inverse_transform(X_train_4s_normalized_KNN),index=X_train_4s_normalized_KNN.index, columns=X_train_4s_normalized_KNN.columns)\n",
        "inverse_X_train_4s_normalized_KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR5ef7bJKgLR"
      },
      "source": [
        "## 8.1.3 Imputation Method #3: MICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIs8cpZdKgLS"
      },
      "source": [
        "%conda install -c conda-forge imbalanced-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0tWzfYbKgLS"
      },
      "source": [
        "\\%pip install fancyimpute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2WA4GWnKgLT"
      },
      "outputs": [],
      "source": [
        "def imputeMICE(df):\n",
        "\n",
        "    # We copy the dataframe to preserve it\n",
        "    df_imputed = df.copy()\n",
        "    # We create object IterativeImputer\n",
        "    mice_imputer = IterativeImputer(sample_posterior=False, random_state=100)\n",
        "    # We make MICE Imputation\n",
        "    df_imputed.iloc[:, :] = mice_imputer.fit_transform(df)\n",
        "    # Return imputed DataFrame\n",
        "    return df_imputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC2AiFBLKgLT"
      },
      "outputs": [],
      "source": [
        "X_train_4s_MICE = imputeMICE(X_train_4s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGHa6Z5yKgLU",
        "outputId": "5bd98d25-aca8-4a3c-c51c-f0a139341742"
      },
      "outputs": [],
      "source": [
        "naCounter(X_train_4s_MICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-qbGraVKgLV",
        "outputId": "0c0713ea-851b-4e19-88b7-fd6a0bfbe532"
      },
      "outputs": [],
      "source": [
        "Y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwAVQhUsKgLW"
      },
      "source": [
        "## 8.2 Evaluation of imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzC2pn2xKgLW"
      },
      "outputs": [],
      "source": [
        "# create dataframe for descriptive analysis\n",
        "descriptive_X_train_4s_normalized_KNN = X_train_4s_normalized_KNN.describe().transpose()\n",
        "\n",
        "# add column for number of unique values of each column\n",
        "descriptive_X_train_4s_normalized_KNN[\"unique\"] = X_train_4s_normalized_KNN.nunique()\n",
        "\n",
        "# add column for outliers\n",
        "outlierls_X_train_4s_normalized_KNN = outliers_z_score(X_train_4s_normalized_KNN,4)\n",
        "descriptive_X_train_4s_normalized_KNN[\"outliers(4s)\"] = outlierls_X_train_4s_normalized_KNN\n",
        "\n",
        "# add column for variance of each column\n",
        "descriptive_X_train_4s_normalized_KNN[\"coeff_var\"] = descriptive_X_train_4s_normalized_KNN[\"std\"]/np.absolute(descriptive_X_train_4s_normalized_KNN[\"mean\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create dataframe for descriptive analysis\n",
        "descriptive_inverse_X_train_4s_normalized_KNN = inverse_X_train_4s_normalized_KNN.describe().transpose()\n",
        "\n",
        "# add column for number of unique values of each column\n",
        "descriptive_inverse_X_train_4s_normalized_KNN[\"unique\"] = X_train_4s_normalized_KNN.nunique()\n",
        "\n",
        "# add column for outliers\n",
        "outlierls_inverse_X_train_4s_normalized_KNN = outliers_z_score(X_train_4s_normalized_KNN,4)\n",
        "descriptive_inverse_X_train_4s_normalized_KNN[\"outliers(4s)\"] = outlierls_inverse_X_train_4s_normalized_KNN\n",
        "\n",
        "# add column for variance of each column\n",
        "descriptive_inverse_X_train_4s_normalized_KNN[\"coeff_var\"] = descriptive_inverse_X_train_4s_normalized_KNN[\"std\"]/np.absolute(descriptive_inverse_X_train_4s_normalized_KNN[\"mean\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26vi1RsWKgLW"
      },
      "outputs": [],
      "source": [
        "# create dataframe for descriptive analysis\n",
        "descriptive_X_train_4s_MICE = X_train_4s_MICE.describe().transpose()\n",
        "\n",
        "# add column for percentage of missing values of each column\n",
        "descriptive_X_train_4s_MICE[\"missing_percentage\"] = X_train_4s_MICE.isnull().sum() * 100 / len(X_train_4s_MICE)\n",
        "\n",
        "# add column for number of unique values of each column\n",
        "descriptive_X_train_4s_MICE[\"unique\"] = X_train_4s_MICE.nunique()\n",
        "\n",
        "# add column for outliers\n",
        "outlierls_train_4s_mice = outliers_z_score(X_train_4s_MICE,4)\n",
        "descriptive_X_train_4s_MICE[\"outliers(4s)\"] = outlierls_train_4s_mice\n",
        "\n",
        "# add column for variance of each column\n",
        "descriptive_X_train_4s_MICE[\"coeff_var\"] = descriptive_X_train_4s_MICE[\"std\"]/np.absolute(descriptive_X_train_4s_MICE[\"mean\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "_erxJF25KgLX",
        "outputId": "4ad9345c-8b0a-41cf-d7ee-a296504d3820"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,8))\n",
        "outliers_chart_X_train_4s_normalized_KNN = sns.histplot(descriptive_X_train_4s_normalized_KNN, x=\"outliers(4s)\", binwidth=5)\n",
        "outliers_chart_X_train_4s_normalized_KNN.set_title('Outliers of Feature Train Set After Outlier Treatment & Missing Value Imputation (KNN)', fontdict={'size':24})\n",
        "outliers_chart_X_train_4s_normalized_KNN.set_xlabel('Number of Outliers',fontdict={'size':15})\n",
        "outliers_chart_X_train_4s_normalized_KNN.set_ylabel('Frequency', fontdict={'size':15})\n",
        "\n",
        "for c in outliers_chart_X_train_4s_normalized_KNN.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    outliers_chart_X_train_4s_normalized_KNN.bar_label(c, labels=labels, fontsize=12, padding=3)\n",
        "\n",
        "\n",
        "plt.xlim(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "ItjG8T1HKgLX",
        "outputId": "250926f0-961b-43a6-9437-1106606b35c6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,8))\n",
        "outliers_chart_X_train_4s_MICE = sns.histplot(descriptive_X_train_4s_MICE, x=\"outliers(4s)\", binwidth=5)\n",
        "outliers_chart_X_train_4s_MICE.set_title('Outliers of Feature Train Set After Outlier Treatment & Missing Value Imputation (MICE)', fontdict={'size':24})\n",
        "outliers_chart_X_train_4s_MICE.set_xlabel('Number of Outliers',fontdict={'size':15})\n",
        "outliers_chart_X_train_4s_MICE.set_ylabel('Frequency', fontdict={'size':15})\n",
        "\n",
        "for c in outliers_chart_X_train_4s_MICE.containers:\n",
        "\n",
        "    # customize the label to account for cases when there might not be a bar section\n",
        "    labels = [f'{h:0.0f}' if (h := v.get_height()) != 0 else '' for v in c ]\n",
        "\n",
        "    # set the bar label\n",
        "    outliers_chart_X_train_4s_MICE.bar_label(c, labels=labels, fontsize=12, padding=3)\n",
        "\n",
        "plt.xlim(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdMv9k1SKgLZ",
        "outputId": "1eb63c3d-5f50-4b1a-8419-7877185cf286"
      },
      "outputs": [],
      "source": [
        "descriptive_normalized_X_train_4s[\"missing_percentage\"].sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "C0mkzXiwKgLZ",
        "outputId": "40b737b3-9943-472a-c3e4-06c72fbfa5fc"
      },
      "outputs": [],
      "source": [
        "# Visualization of a variable with most missing values\n",
        "plt.figure(figsize = (20,12))\n",
        "X_train_4s['feature345'].plot(kind='kde',c='red',linewidth=3)\n",
        "inverse_X_train_4s_normalized_KNN['feature345'].plot(kind='kde')\n",
        "labels = ['Baseline', 'KNN','MICE']\n",
        "plt.legend(labels)\n",
        "plt.xlabel('feature345')\n",
        "plt.gca().set(title='Density plot of feature345');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization of a variable with most missing values\n",
        "plt.figure(figsize = (20,12))\n",
        "X_train_4s['feature346'].plot(kind='kde',c='red',linewidth=3)\n",
        "X_train_4s_MICE['feature346'].plot(kind='kde')\n",
        "labels = ['Baseline','MICE']\n",
        "plt.legend(labels)\n",
        "plt.xlabel('feature346')\n",
        "plt.gca().set(title='Density plot of feature345');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io1JWUiyKgLZ"
      },
      "outputs": [],
      "source": [
        "# Visualization of a variable with most missing values\n",
        "plt.figure(figsize = (20,12))\n",
        "X_train_4s['feature346'].plot(kind='kde',c='red',linewidth=3)\n",
        "inverse_X_train_4s_normalized_KNN['feature346'].plot(kind='kde')\n",
        "X_train_4s_MICE['feature346'].plot(kind='kde')\n",
        "labels = ['Baseline', 'KNN','MICE']\n",
        "plt.legend(labels)\n",
        "plt.xlabel('feature346')\n",
        "plt.gca().set(title='Density plot of feature346')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "id": "NV-10g93KgLa",
        "outputId": "484ec521-e9a7-46ae-9ad1-dd331e5a576d"
      },
      "outputs": [],
      "source": [
        "# Visualization of a variable with most missing values\n",
        "plt.figure(figsize = (20,12))\n",
        "X_train_4s['feature346'].plot(kind='kde',c='red',linewidth=3)\n",
        "inverse_X_train_4s_normalized_KNN['feature346'].plot(kind='kde')\n",
        "X_train_4s_MICE['feature346'].plot(kind='kde')\n",
        "labels = ['Baseline', 'KNN','MICE']\n",
        "plt.legend(labels)\n",
        "plt.xlabel('feature580')\n",
        "plt.gca().set(title='Density plot of feature feature346');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwMi1lZrKgLa"
      },
      "outputs": [],
      "source": [
        "corr_X_train_4s = X_train_4s.corr()\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize = (20,8))\n",
        "correlation_heatmap_X_train_4s= sns.heatmap(corr_X_train_4s, annot=False)\n",
        "correlation_heatmap_X_train_4s.set_title('Correlation Heatmap of Features (Before Imputation of Missing Values)', fontdict={'size':24})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyXeCR2sKgLb"
      },
      "outputs": [],
      "source": [
        "corr_X_train_4s_normalized_KNN = X_train_4s_normalized_KNN.corr()\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize = (20,8))\n",
        "corr_X_train_4s_normalized_KNN= sns.heatmap(corr_X_train_4s_normalized_KNN, annot=False)\n",
        "corr_X_train_4s_normalized_KNN.set_title('Correlation Heatmap of Features (After KNN Imputation)', fontdict={'size':24})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM32w-C2KgLb"
      },
      "outputs": [],
      "source": [
        "corr_X_train_4s_MICE = X_train_4s_MICE.corr()\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize = (20,8))\n",
        "correlation_heatmap_X_train_4s_MICE= sns.heatmap(corr_X_train_4s_MICE, annot=False)\n",
        "correlation_heatmap_X_train_4s_MICE.set_title('Correlation Heatmap of Features (After MICE Imputation)', fontdict={'size':24})\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xAZqiIifKgLc"
      },
      "source": [
        "Since we have 450 remaining features, it is necessary find alternative methods to reduce the total number of features previous to the model implementation in order to get better computer performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARpuGqzDKgLc"
      },
      "source": [
        "## 9.1. Feature Selection Method #1: BORUTA (Wrapper)\n",
        "\n",
        "Boruta is a feature selection algorithm.\n",
        "\n",
        "It works as a wrapper algorithm around Random Forest.\n",
        "\n",
        "In Boruta, features do not compete with one another. Instead, they compete against a randomized version of themselves called 'shadow features'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gr8manwKgLc"
      },
      "outputs": [],
      "source": [
        "def implementBoruta(df_X, df_y):\n",
        "\n",
        "    DFx = df_X\n",
        "    DFy = df_y\n",
        "\n",
        "    df_X = df_X.values\n",
        "    df_y = df_y.values\n",
        "    # We need to create a Random Forest Classifier\n",
        "    rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=100)\n",
        "\n",
        "    # We create the object Boruta\n",
        "    boruta = BorutaPy(rf, n_estimators='auto', random_state=100, max_iter=100)\n",
        "\n",
        "    # We start the process of selection of features\n",
        "    boruta.fit(df_X, df_y)\n",
        "\n",
        "    # We concatenate both sets in df_train\n",
        "    df_train = pd.concat([DFx, DFy], axis=1)\n",
        "\n",
        "    # We get the list of selected fratures\n",
        "    selected = df_train.columns[:-1][boruta.support_].tolist()\n",
        "\n",
        "    borutafeatures_df = DFx[selected]\n",
        "\n",
        "    print(selected)\n",
        "    return borutafeatures_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY4oxropKgLd"
      },
      "outputs": [],
      "source": [
        "Y_train_reset_encoded = Y_train_reset.replace({\"PASS\":0, \"FAIL\":1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iEmd6dmiLv6Z",
        "outputId": "6c319d2a-fc80-4513-de72-f5736e62fa80"
      },
      "outputs": [],
      "source": [
        "# boruta feature selection from trainset not scaled and imputed with KNN\n",
        "borutafeatures_X_train_4s_normalized_KNN = implementBoruta(X_train_4s_normalized_KNN, Y_train_reset)\n",
        "borutafeatures_X_train_4s_normalized_KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VfkrdJewp9zd",
        "outputId": "49297896-e945-4457-982f-3717d716ba06"
      },
      "outputs": [],
      "source": [
        "# boruta feature selection from trainset not scaled and imputed with KNN\n",
        "borutafeatures_X_train_4s_MICE = implementBoruta(X_train_4s_MICE, Y_train)\n",
        "\n",
        "# dataframe of features selected\n",
        "pd.DataFrame(borutafeatures_X_train_4s_MICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yVzBX5pqHcF",
        "outputId": "c311841c-7c69-4ca7-a0ae-4788e29e7489"
      },
      "outputs": [],
      "source": [
        "Y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HkQEXfKKgLf"
      },
      "source": [
        "## 9.2. Feature Selection method #2: RFE (Wrapper)\n",
        "\n",
        "RFE is a greedy optimization technique that looks for the highest performing feature subset.\n",
        "\n",
        "It produces models over and over again, putting aside the best or worst performing feature at each iteration.\n",
        "\n",
        "It builds the next model using the leftover features until all of the features are used up.\n",
        "\n",
        "The features are then ranked based on the order of their elimination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-vnxbH7KgLg"
      },
      "outputs": [],
      "source": [
        "def implementRFE(df_X, df_y):\n",
        "\n",
        "    DFx = df_X\n",
        "    DFy = df_y\n",
        "\n",
        "    df_X = df_X.values\n",
        "    df_y = df_y.values\n",
        "    # We need to create a Random Forest Classifier\n",
        "    rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=100)\n",
        "\n",
        "    # We create the object RFE\n",
        "    rfe = RFE(estimator = rf, n_features_to_select=20, step=10)\n",
        "\n",
        "    # We start the process of selection of features\n",
        "    rfe.fit(df_X, df_y)\n",
        "\n",
        "    # We concatenate both sets in df_train\n",
        "    df_train = pd.concat([DFx, DFy], axis=1)\n",
        "\n",
        "    # We get the list of selected fratures\n",
        "    selected = df_train.columns[:-1][rfe.support_].tolist()\n",
        "    print(selected)\n",
        "    return selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "rGapOzkMKgLg",
        "outputId": "6e6d47e9-d337-4ec8-9642-7fc44ef372d6"
      },
      "outputs": [],
      "source": [
        "# RFE feature selection from trainset imputed with KNN\n",
        "RFEfeatures_X_train_4s_normalized_KNN = implementRFE(X_train_4s_normalized_KNN, Y_train_reset)\n",
        "\n",
        "# dataframe of features selected\n",
        "pd.DataFrame(RFEfeatures_X_train_4s_normalized_KNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "Kg6bm3VDKgLh",
        "outputId": "366667f7-85f5-4ec4-9acc-76a6f8c20552"
      },
      "outputs": [],
      "source": [
        "# RFE feature selection from trainset imputed with MICE\n",
        "RFEfeatures_X_train_4s_MICE = implementRFE(X_train_4s_MICE, Y_train)\n",
        "\n",
        "# dataframe of features selected\n",
        "pd.DataFrame(RFEfeatures_X_train_4s_MICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWxQ9yp0KgLh"
      },
      "source": [
        "## 9.3. Feature Selection #3: LASSO (Embedded Method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4ctyH3gKgLi"
      },
      "outputs": [],
      "source": [
        "# Assuming you have your features stored in X and the target variable in y\n",
        "\n",
        "def implementLasso(X, y):\n",
        "    # Create Lasso regression model\n",
        "    lasso = Lasso(alpha=0.1)  # Set the regularization parameter alpha\n",
        "\n",
        "    # Perform feature selection using Lasso\n",
        "    feature_selector = SelectFromModel(lasso)\n",
        "    selected_features = feature_selector.fit_transform(X, y)\n",
        "\n",
        "    # Get the selected feature indices\n",
        "    feature_indices = feature_selector.get_support(indices=True)\n",
        "\n",
        "    # Print the selected feature names\n",
        "    selected_feature_names = X.columns[feature_indices]\n",
        "    print(\"Selected features:\", selected_feature_names)\n",
        "\n",
        "    return selected_feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iv_Ywp6KgLi"
      },
      "outputs": [],
      "source": [
        "# Give text labels to the training examples\n",
        "Y_train_encoded = Y_train.replace({\"PASS\":0, \"FAIL\":1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "id": "tV98hpoCKgLi",
        "outputId": "681fefef-8365-4041-8cf1-7ce1871f12b3"
      },
      "outputs": [],
      "source": [
        "# Lasso feature selection from trainset imputed with KNN\n",
        "Lassofeatures_X_train_4s_normalized_KNN = implementLasso(X_train_4s_normalized_KNN, Y_train_reset_encoded)\n",
        "\n",
        "# dataframe of features selected\n",
        "pd.DataFrame(Lassofeatures_X_train_4s_normalized_KNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k941dlcvKgLj",
        "outputId": "8a850ab3-bb62-4613-9c8e-2a86c8773ff1"
      },
      "outputs": [],
      "source": [
        "# Give text labels to the training examples\n",
        "Y_train_encoded = Y_train.replace({\"PASS\":0, \"FAIL\":1})\n",
        "\n",
        "# Lasso feature selection from trainset imputed with MICE\n",
        "Lassofeatures_X_train_4s_MICE = implementLasso(X_train_4s_MICE, Y_train_reset_encoded)\n",
        "\n",
        "# dataframe of features selected\n",
        "pd.DataFrame(Lassofeatures_X_train_4s_MICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tykTqesKgLl"
      },
      "source": [
        "# Correlation Matrix of Selected Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "YqAsWKxv6MJH",
        "outputId": "c4a31207-b4b0-413f-83d1-75ccf938bc9f"
      },
      "outputs": [],
      "source": [
        "corr_borutafeatures_X_train = X_train[borutafeatures_X_train_4s_normalized_KNN].corr()\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize = (20,8))\n",
        "heatmap_borutafeatures_X_train= sns.heatmap(corr_borutafeatures_X_train, annot=True)\n",
        "heatmap_borutafeatures_X_train.set_title('Correlation Heatmap of Features with BORUTA (Before Outlier Treatment)', fontdict={'size':24})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "WyqJUYXhuHku",
        "outputId": "fc3f0a9a-e04d-41f8-be48-0fc7538e7269"
      },
      "outputs": [],
      "source": [
        "corr_borutafeatures_X_train_4s = X_train_4s[borutafeatures_X_train_4s_normalized_KNN].corr()\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize = (20,8))\n",
        "heatmap_borutafeatures_X_train_4s= sns.heatmap(corr_borutafeatures_X_train_4s, annot=True)\n",
        "heatmap_borutafeatures_X_train_4s.set_title('Correlation Heatmap of Features with BORUTA (After Outlier Treatment)', fontdict={'size':24})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "T_hKMHb_2GKm",
        "outputId": "fd473d97-2928-42bc-e88e-daf4b31dac1a"
      },
      "outputs": [],
      "source": [
        "corr_borutafeatures_X_train_4s_normalized_KNN = X_train_4s_normalized_KNN[borutafeatures_X_train_4s_normalized_KNN].corr()\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize = (20,8))\n",
        "correlation_heatmap_borutafeatures_X_train_4s_normalized_KNN= sns.heatmap(corr_borutafeatures_X_train_4s_normalized_KNN, annot=True)\n",
        "correlation_heatmap_borutafeatures_X_train_4s_normalized_KNN.set_title('Correlation Heatmap of Features with BORUTA (After KNN)', fontdict={'size':24})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4nfP8BUKgLn"
      },
      "source": [
        "# 10. Class Balancing Methods   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfolH6T_KgLn",
        "outputId": "61773246-9fef-4ed7-9e18-6028d3fd2b37"
      },
      "outputs": [],
      "source": [
        "Y_train.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-PhF5dQKgLn"
      },
      "source": [
        "## 10.1 Balancing Method #1: SMOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VWpYjVrKgLo"
      },
      "source": [
        "### 10.1.1 Knn + Boruta + SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYmrZgSaKgLo"
      },
      "outputs": [],
      "source": [
        "oversampler = SMOTE(random_state=88)\n",
        "X_train_normalized_smote_boruta_knn, Y_train_smote = oversampler.fit_resample(X_train_4s_normalized_KNN[borutafeatures_X_train_4s_normalized_KNN], Y_train_reset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy3jijqWKgLo",
        "outputId": "e4b817fe-0c6d-469a-c441-2faebd643587"
      },
      "outputs": [],
      "source": [
        "Y_train_smote.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "ERBtqgXGKgLp",
        "outputId": "2ccfa694-0ec1-4c14-cba8-696cd5dae1fe"
      },
      "outputs": [],
      "source": [
        "corr_X_train_normalized_smote_boruta_knn = X_train_normalized_smote_boruta_knn.corr()\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize = (20,8))\n",
        "correlation_heatmap_X_train_normalized_smote_boruta_knn = sns.heatmap(corr_X_train_normalized_smote_boruta_knn, annot=True)\n",
        "correlation_heatmap_X_train_normalized_smote_boruta_knn.set_title('Correlation Heatmap of Features (Knn + Boruta + SMOTE)', fontdict={'size':24})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMka8H3VKgLp"
      },
      "source": [
        "### 10.1.1 Mice + Boruta + SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejQnmeqHKgLq"
      },
      "outputs": [],
      "source": [
        "oversampler = SMOTE(random_state=88)\n",
        "X_train_normalized_smote_boruta_mice, Y_train_smote = oversampler.fit_resample(X_train_4s_MICE[borutafeatures_X_train_4s_MICE], Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hyeDKlIKgLq"
      },
      "source": [
        "## 10.2. Balancing Method #2: ROSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfogIKp9KgLq"
      },
      "source": [
        "### 10.2.1 Knn + Boruta + ROSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbKOnkCtKgLs"
      },
      "outputs": [],
      "source": [
        "\n",
        "rose = RandomOverSampler(random_state=88)\n",
        "X_train_normalized_rose_boruta_knn, Y_train_rose = rose.fit_resample(X_train_4s_normalized_KNN[borutafeatures_X_train_4s_normalized_KNN], Y_train_reset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDoWxAkkKgLs"
      },
      "outputs": [],
      "source": [
        "Y_train_rose.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg8fds4QKgLt"
      },
      "outputs": [],
      "source": [
        "corr = X_train_normalized_rose_boruta_knn.corr()\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize = (20,8))\n",
        "correlation_heatmap_X_train_rose = sns.heatmap(corr, annot=True)\n",
        "correlation_heatmap_X_train_rose.set_title('Correlation Heatmap of Features (Knn + Boruta + ROSE)', fontdict={'size':24})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkiB2VgoKgLu"
      },
      "source": [
        "# 11. Model Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sHc9isuKgLu"
      },
      "source": [
        "### Preprocessing Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop constant columns and columns having missing values >50 as the same with Train Set\n",
        "\n",
        "X_test.drop(constant_columns,axis=1,inplace=True)\n",
        "X_test.drop(missing_50_col_list, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJIPTKkKKgLu"
      },
      "outputs": [],
      "source": [
        "# We apply the same outlier treatment but over TEST dataset\n",
        "X_test_2 = outliers_treatment(X_test,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qNB-8DsKgLv"
      },
      "outputs": [],
      "source": [
        "# Since we are going to use KNN imputation method for missing values\n",
        "# We apply the same Standard Scaling to the X_test_scaled\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_test_2)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test_2), index=X_test_2.index, columns=X_test_2.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDfRTtekKgLv"
      },
      "outputs": [],
      "source": [
        "# Over X_test_scaled we apply KNN imputation\n",
        "X_test_imputed_KNN = imputeKNN(X_test_scaled, 5)\n",
        "# Since KNN imputation restart indexes over training dataset,\n",
        "# We apply the reset_index to the test target values dataset too\n",
        "Y_test_reset = Y_test.reset_index(drop=True)\n",
        "# Then, we filter to only get the features selected by Boruta+KNN in the previous step.\n",
        "X_test_final = X_test_imputed_KNN[borutafeatures_X_train_4s_normalized_KNN]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USFv-1foKgLw"
      },
      "source": [
        "### Prediction & Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 1: in order to organize the information better, we rename the variables with the treated datasets.\n",
        "X_test = X_test_final\n",
        "y_train = Y_train_smote\n",
        "y_test = Y_test_reset\n",
        "\n",
        "# STEP 2: We create and train the RandomForestClassifier model\n",
        "randfor = RandomForestClassifier(n_jobs=-1, max_depth=5, random_state=100)\n",
        "randfor.fit(X_train_normalized_smote_boruta_knn, y_train)\n",
        "\n",
        "# STEP 3: We create the predictor to use it over the test dataset\n",
        "y_pred = randfor.predict(X_test_final)\n",
        "\n",
        "# STEP 4: We convert the tags in numeric values according to the libraries requirements\n",
        "y_test_numeric = np.where(y_test == 'FAIL', 1, 0)\n",
        "y_pred_numeric = np.where(y_pred == 'FAIL', 1, 0)\n",
        "\n",
        "# STEP 5: Now we convert this values into dataframes also accordint to libraries requirements\n",
        "y_pred = pd.DataFrame(y_pred)\n",
        "y_test_numeric = pd.DataFrame(y_test_numeric)\n",
        "y_pred_numeric = pd.DataFrame(y_pred_numeric)\n",
        "\n",
        "# STEP 6: We calculate the confusion matrix and print it\n",
        "confusion = confusion_matrix(y_test_numeric, y_pred_numeric)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion)\n",
        "\n",
        "# STEP 7: We calculate accuracy and print it\n",
        "accuracy = accuracy_score(y_test_numeric, y_pred_numeric)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# STEP 8: We calculate precision and print it\n",
        "precision = precision_score(y_test_numeric, y_pred_numeric)\n",
        "print(\"Precision:\", precision)\n",
        "\n",
        "# STEP 9: We calculate Recall index and print it\n",
        "recall = recall_score(y_test_numeric, y_pred_numeric)\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# STEP 10: We calculate F1-score and print it\n",
        "f1 = f1_score(y_test_numeric, y_pred_numeric)\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "# STEP 11: We calculate Area Under the Curve of ROC score (ROC-AUC) and print it\n",
        "roc_auc = roc_auc_score(y_test_numeric, y_pred_numeric)\n",
        "print(\"ROC AUC:\", roc_auc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GridSearch for parameteres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define your Boruta transformer\n",
        "class BorutaFeatureSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, estimator, n_estimators=100, random_state=None):\n",
        "        self.estimator = estimator\n",
        "        self.n_estimators = n_estimators\n",
        "        self.random_state = random_state\n",
        "        self.selector = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.selector = BorutaPy(estimator=self.estimator,\n",
        "                                 n_estimators=self.n_estimators,\n",
        "                                 random_state=self.random_state)\n",
        "        self.selector.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X[:, self.selector.support_]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encode Label train and test into numeric ones\n",
        "Y_test_encoded = Y_test.replace({\"PASS\":0, \"FAIL\":1})\n",
        "Y_train_encoded = Y_train.replace({\"PASS\":0, \"FAIL\":1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the pipeline with imbalance handling, imputation, scaling, and classifier\n",
        "pipeline_knn = Pipeline([\n",
        "\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('imputer', KNNImputer()),\n",
        "    ('selector', BorutaFeatureSelector(estimator=RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=100))),\n",
        "    ('sampler', SMOTE()),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid = {'selector': [BorutaPy(estimator=RandomForestClassifier()), RFE(estimator=RandomForestClassifier())],\n",
        "              'sampler': [SMOTE(), RandomOverSampler()],\n",
        "    'classifier': [RandomForestClassifier(), SVC(), LogisticRegression(), XGBClassifier()],\n",
        "    'imputer__n_neighbors': [4,5,6],\n",
        "    'imputer__weights': ['uniform', 'distance'],\n",
        "    'sampler__random_state': [42,100]\n",
        "}\n",
        "\n",
        "\n",
        "# Create the grid search object\n",
        "grid_search = GridSearchCV(pipeline_knn, param_grid, cv=5, scoring='f1')\n",
        "\n",
        "# Fit the grid search object on the data\n",
        "grid_search.fit(X_train_4s, Y_train_encoded)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Fit the pipeline with the best parameters on the full training data\n",
        "pipeline_knn.set_params(**best_params)\n",
        "pipeline_knn.fit(X_train_4s, Y_train_encoded)\n",
        "\n",
        "# Evaluate the pipeline on the test data\n",
        "accuracy = pipeline_knn.score(X_test_2, Y_test_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataframe from gridsearch results\n",
        "result_df = pd.DataFrame.from_dict(grid_search.cv_results_, orient='columns')\n",
        "result_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipeline Prediction Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pipeline_results(pipeline, xtest, ytest, confusion_title):\n",
        "    \n",
        "    # STEP 1: in order to organize the information better, we rename the variables with the treated datasets.\n",
        "    x_test = xtest\n",
        "    y_test = ytest\n",
        "\n",
        "\n",
        "    y_pred = pipeline.predict(x_test)\n",
        "\n",
        "    # STEP 5: Now we convert this values into dataframes also accordint to libraries requirements\n",
        "    y_pred_numeric = pd.DataFrame(y_pred)\n",
        "    y_test_numeric = pd.DataFrame(y_test)\n",
        "\n",
        "\n",
        "    # STEP 6: We calculate the confusion matrix and print it\n",
        "    confusion = confusion_matrix(y_test_numeric, y_pred_numeric)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion)\n",
        "\n",
        "    # STEP 7: We calculate accuracy and print it\n",
        "    accuracy = accuracy_score(y_test_numeric, y_pred_numeric)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "\n",
        "    # STEP 8: We calculate precision and print it\n",
        "    precision = precision_score(y_test_numeric, y_pred_numeric)\n",
        "    print(\"Precision:\", precision)\n",
        "\n",
        "    # STEP 9: We calculate Recall index and print it\n",
        "    recall = recall_score(y_test_numeric, y_pred_numeric)\n",
        "    print(\"Recall:\", recall)\n",
        "\n",
        "    # STEP 10: We calculate F1-score and print it\n",
        "    f1 = f1_score(y_test_numeric, y_pred_numeric)\n",
        "    print(\"F1-score:\", f1)\n",
        "\n",
        "    # STEP 11: We calculate Area Under the Curve of ROC score (ROC-AUC) and print it\n",
        "    roc_auc = roc_auc_score(y_test_numeric, y_pred_numeric)\n",
        "    print(\"ROC AUC:\", roc_auc)\n",
        "\n",
        "    # STEP 12: To visualize the confusion matrix, we create a heatmap using the existent matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\",xticklabels=[\"PASS\", \"FAIL\"], yticklabels=[\"PASS\", \"FAIL\"])\n",
        "    plt.title(confusion_title)\n",
        "    plt.xlabel(\"Predicted Values\")\n",
        "    plt.ylabel(\"Actual Values\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### KNN+BORUTA+SMOTE+RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the pipeline with imbalance handling, imputation, scaling, and classifier\n",
        "pipeline_knn_boruta_smote_RandomForestClassifier = Pipeline([\n",
        "\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('imputer', KNNImputer(n_neighbors=5,weights=\"uniform\")),\n",
        "    ('selector', BorutaFeatureSelector(estimator=RandomForestClassifier())),\n",
        "    ('sampler', SMOTE(random_state=42)),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "pipeline_knn_boruta_smote_RandomForestClassifier.fit(X_train_4s, Y_train_encoded)\n",
        "\n",
        "pipeline_results(pipeline_knn_boruta_smote_RandomForestClassifier, X_test_2, Y_test_encoded, \"Confusion Matrix - KNN+BORUTA+SMOTE+RandomForestClassifier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  KNN+RFE+SMOTE+SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the pipeline with imbalance handling, imputation, scaling, and classifier\n",
        "pipeline_knn_boruta_smote_SVC = Pipeline([\n",
        "\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('imputer', KNNImputer(n_neighbors=4,weights=\"uniform\")),\n",
        "    ('selector', RFE(estimator=RandomForestClassifier())),\n",
        "    ('sampler', SMOTE(random_state=100)),\n",
        "    ('classifier', SVC())\n",
        "])\n",
        "\n",
        "\n",
        "pipeline_knn_boruta_smote_SVC.fit(X_train_4s, Y_train_encoded)\n",
        "\n",
        "pipeline_results(pipeline_knn_boruta_smote_SVC, X_test_2, Y_test_encoded, \"Confusion Matrix - KNN+RFE+SMOTE+SVC\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### KNN+RFE+SMOTE+LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the pipeline with imbalance handling, imputation, scaling, and classifier\n",
        "pipeline_knn_boruta_smote_LogisticRegression = Pipeline([\n",
        "\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('imputer', KNNImputer(n_neighbors=4,weights=\"distance\")),\n",
        "    ('selector', RFE(estimator=RandomForestClassifier())),\n",
        "    ('sampler', SMOTE(random_state=100)),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "\n",
        "pipeline_knn_boruta_smote_LogisticRegression.fit(X_train_4s, Y_train_encoded)\n",
        "\n",
        "pipeline_results(pipeline_knn_boruta_smote_LogisticRegression, X_test_2, Y_test_encoded, \"Confusion Matrix - KNN+RFE+SMOTE+LogisticRegression\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### KNN+BORUTA+SMOTE+XGB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the pipeline with imbalance handling, imputation, scaling, and classifier\n",
        "pipeline_knn_boruta_smote_XGB = Pipeline([\n",
        "\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('imputer', KNNImputer(n_neighbors=4,weights=\"distance\")),\n",
        "    ('selector', BorutaFeatureSelector(estimator=RandomForestClassifier(), random_state=42)),\n",
        "    ('sampler', SMOTE(random_state=42)),\n",
        "    ('classifier', XGBClassifier(seed = 42 , objective = 'binary:logistic', missing = 0))\n",
        "])\n",
        "\n",
        "pipeline_knn_boruta_smote_XGB.fit(X_train_4s, Y_train_encoded)\n",
        "\n",
        "pipeline_results(pipeline_knn_boruta_smote_XGB, X_test_2, Y_test_encoded, \"Confusion Matrix - KNN+BORUTA+SMOTE+XGB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the pipeline with imbalance handling, imputation, scaling, and classifier\n",
        "pipeline_knn_Lasso = Pipeline([\n",
        "\n",
        "    ('imputer', KNNImputer()),\n",
        "    ('selector', SelectFromModel(Lasso(random_state=42,normalize=True))),\n",
        "    ('sampler', SMOTE()),\n",
        "    ('classifier', SVC())\n",
        "])\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid = {\n",
        "    'sampler': [SMOTE(), RandomOverSampler()],\n",
        "    'classifier': [RandomForestClassifier(), SVC(),LogisticRegression(),XGBClassifier()],\n",
        "    'imputer__n_nearest_features': [4,5,6],\n",
        "    'sampler__random_state': [42,100]\n",
        "}\n",
        "\n",
        "# Create the grid search object\n",
        "grid_search_knn_Lasso = GridSearchCV(pipeline_knn_Lasso, param_grid, cv=5, scoring='f1',n_jobs=-1)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
